# Tableau r√©capitulatif complet des flux - Exercice 1 InduTechData

## Architecture finale CORRIG√âE (7 flux critiques)

| # | Source | Destination | Type | Criticit√© | **Donn√©es transf√©r√©es** | **Volume** | **Fr√©quence** | **Protocole/Outil** | **S√©curit√©** | **Raison / Justification** |
|---|--------|-------------|------|-----------|---|---|---|---|---|---|
| **1** | SAN | Redpanda | ‚ö° Temps r√©el | Critique | **Flux IoT brut** (capteurs industriels, 50 Go/mois) | ~20 MB/jour | Continu (temps r√©el) | Kafka API + connecteur | SSL/TLS 1.3 (port 9093) | Ingestion donn√©es temps r√©el IoT (besoin nouveau 50 Go/mois mentionn√© exercice) |
| **2** | Redpanda | S3 | ‚ö° Temps r√©el | **Critique** | **Donn√©es IoT transform√©es** (Parquet, compress√©es) | ~50 GB/mois | Micro-batches (toutes les 5-10 min) | S3 API HTTPS + PySpark Streaming | HTTPS + AES-256 S3 server-side encryption | **BACKUP IoT brut** = data lake source de v√©rit√© pour donn√©es temps r√©el |
| **3** | Redpanda | Redshift | ‚ö° Temps r√©el | Non-critique | **Agr√©gations temps r√©el** (SUM revenue, COUNT tickets, avg latency) | Variable (~100 MB/min) | Continu (streaming writes) | Direct Redshift API (JDBC) | SSL/TLS over JDBC + IAM auth | Analytical queries temps r√©el (optionnel, Redshift moteur calcul, pas stockage) |
| **4** | SQL Server | S3 | üì¶ Batch | **Critique** | **Donn√©es m√©tier BRUTES** (40 To ERP/CRM : orders, customers, products, invoices) | 40 TB initial + delta daily (~200 GB/day) | Initial migration (1 √ó 3-7 jours) + nightly delta sync | AWS DataSync ou AWS DMS export | SSL/TLS + AES-256 S3 encryption | **BACKUP m√©tier brut** = source de v√©rit√© donn√©es critiques m√©tier (on-premise) |
| **5** | SQL Server | Redshift | üì¶ Batch | Non-critique | **Donn√©es m√©tier transform√©es** (40 To d√©normalis√©es pour analytics) | 40 TB initial + delta daily (~200 GB/day) | Initial load (1 √ó 3-7 jours) + nightly incremental (via DMS CDC) | AWS DMS (Database Migration Service) + COPY command | JDBC over SSL/TLS + AES-256 Redshift encryption | Analytical data warehouse (Redshift = moteur analytique, pas backup primaire) |
| **6** | Active Directory | AWS Directory Service | üîÑ Temps r√©el | Critique | **M√©tadonn√©es identit√©** (users, groups, permissions, DN entries) | ~50 MB (m√©tadonn√©es uniquement) | Bi-directionnel, temps r√©el (sync continu) | LDAP over SSL/TLS (port 636) + AWS AD Replication | SSL/TLS + Kerberos auth | SSO unifi√© on-prem ‚Üî cloud (identit√©s critiques = m√™me permissions partout) |
| **7** | Services (Redpanda, Redshift, S3, Directory Service) | CloudWatch | üëÅÔ∏è Temps r√©el | Non-critique | **M√©triques & Logs monitoring** (CPU %, memory, queries/sec, errors, latency) | ~5-10 GB/mois (logs agr√©g√©s) | Continu (1min intervals pour m√©triques) | HTTPS CloudWatch API + CloudTrail | HTTPS + IAM IAM roles | Observabilit√© infrastructure (non-critique = ne pas compris alertes de sauvegarde) |

---

## D√©tails compl√©mentaires par flux

### FLUX 1 : SAN ‚Üí Redpanda (‚ö° Temps r√©el - Critique)

| Aspect | Description |
|--------|-------------|
| **Source** | SAN Storage (10 To) = donn√©es non-structur√©es |
| **Donn√©es** | Flux IoT brut (capteurs industriels temps r√©el) |
| **Volume** | 50 Go/mois = ~20 MB/jour = ~15 KB/sec (tr√®s faible) |
| **Fr√©quence** | Continu (√©v√©nementiel, pas batch) |
| **Protocole** | Kafka API (Redpanda Kafka-compatible) |
| **Port** | 9093 (SSL/TLS) ou 9092 (plaintext) |
| **Chiffrage** | SSL/TLS 1.3, certificats mutuels |
| **Authentification** | SASL/SCRAM ou IAM (si AWS MSK) |
| **Outil** | Kafka Connect source ou collecteur custom (Filebeat, Fluentd) |
| **Latence acceptable** | < 1 seconde (IoT critique) |
| **RTO/RPO** | RPO < 1 sec, RTO < 5 min (donn√©es IoT peuvent √™tre perdues court-terme) |

---

### FLUX 2 : Redpanda ‚Üí S3 (‚ö° Temps r√©el - Critique)

| Aspect | Description |
|--------|-------------|
| **Source** | Redpanda Topics (client_tickets, sensor_data, etc.) |
| **Destination** | S3 Buckets (Raw Data partition) |
| **Donn√©es** | IoT transform√©/agr√©g√© en Parquet (m√™me donn√©es que Flux 1, format optimis√©) |
| **Volume** | ~50 GB/mois (l√©g√®rement compress√© vs JSON brut) |
| **Fr√©quence** | Micro-batches : √©crire un fichier Parquet toutes les 5-10 minutes |
| **Format** | Parquet (colonaire, compress√© ~3x vs JSON) |
| **Partitioning** | S3 path : `s3://indutech-datalake/iot/year=2025/month=01/day=17/hour=16/` |
| **Protocole** | S3 API HTTPS (boto3, Spark S3A) |
| **Chiffrage** | AES-256 server-side encryption (S3 managed or KMS) |
| **Authentification** | IAM roles EC2 (Redpanda runs on EC2) |
| **Outil** | PySpark Streaming WRITE ou custom Kafka ‚Üí S3 connector |
| **Latence acceptable** | < 15 minutes (batch acceptable, data lake) |
| **RTO/RPO** | RPO = 5-10 min (time between writes), RTO = 1h (restore from latest partition) |
| **Justification criticit√©** | C'est le BACKUP de donn√©es IoT brutes ‚Üí source de v√©rit√© long-terme |

---

### FLUX 3 : Redpanda ‚Üí Redshift (‚ö° Temps r√©el - Non-critique)

| Aspect | Description |
|--------|-------------|
| **Source** | Redpanda Topics (client_tickets, sensor_data, etc.) |
| **Destination** | Redshift Cluster (tables analytiques) |
| **Donn√©es** | Agr√©gations temps r√©el (SUM revenue, COUNT tickets, avg latency, group by customer/region) |
| **Volume** | Variable, ~100 MB/min (depends on aggregations & downsampling) |
| **Fr√©quence** | Continu streaming writes via Spark Streaming or Kinesis Firehose |
| **Transformation** | PySpark Streaming : GROUP BY, window functions, enrichments |
| **Protocole** | JDBC (Redshift native driver) |
| **Port** | 5439 (Redshift default) |
| **Chiffrage** | SSL/TLS over JDBC |
| **Authentification** | Redshift IAM credentials + database user/password |
| **Outil** | PySpark Streaming WRITE mode="append" to Redshift |
| **Latence acceptable** | 10-60 seconds (near real-time analytics OK) |
| **RTO/RPO** | RPO N/A (recalculate from S3), RTO = rebuild from Flux 2 data |
| **Justification non-criticit√©** | Redshift = moteur analytique optionnel. Si crash, donn√©es peuvent √™tre recalcul√©es depuis S3 (Flux 2) |

---

### FLUX 4 : SQL Server ‚Üí S3 (üì¶ Batch - Critique)

| Aspect | Description |
|--------|-------------|
| **Source** | SQL Server Cluster (40 To) = ERP/CRM m√©tier |
| **Destination** | S3 Buckets (Backup partition) |
| **Donn√©es** | Dump complet donn√©es m√©tier BRUTES (tables : orders, customers, products, invoices, GL) |
| **Volume** | 40 TB initial migration + ~200 GB delta/day |
| **Fr√©quence** | Initial : 1√ó migration (3-7 jours d√©bit r√©seau) + Nightly incremental (02:00 UTC) |
| **Format** | CSV, Parquet, or Backup files (.bak format convertis en S3) |
| **Partitioning** | S3 path : `s3://indutech-backup/sql-server/year=2025/month=01/day=17/` |
| **Protocole** | HTTPS (S3 API via AWS DataSync or custom scripts) |
| **Chiffrage** | AES-256 server-side encryption + client-side encryption (KMS optional) |
| **Authentification** | IAM role (on AWS migration instance) |
| **Outil** | AWS DataSync (managed service) OR AWS Snowball (for initial 40 TB, then sync) |
| **Latency acceptable** | < 24 hours acceptable (batch daily) |
| **RTO/RPO** | RPO = 24h (daily backup), RTO = 2-4h (restore from latest daily) |
| **Justification criticit√©** | **SOURCE DE V√âRIT√â = donn√©es m√©tier brutes** (on-prem). MUST backup for DR/compliance. Non-negotiable. |

---

### FLUX 5 : SQL Server ‚Üí Redshift (üì¶ Batch - Non-critique)

| Aspect | Description |
|--------|-------------|
| **Source** | SQL Server Cluster (40 To) = ERP/CRM m√©tier |
| **Destination** | Redshift Cluster (tables d√©normalis√©es analytiques) |
| **Donn√©es** | M√™mes donn√©es SQL Server MAIS transform√©es (d√©normalis√©es, agr√©g√©es pour OLAP) |
| **Volume** | 40 TB initial + ~200 GB delta/day |
| **Fr√©quence** | Initial load (1√ó, 3-7 jours) + Nightly incremental sync (02:00 UTC) |
| **Transformation** | ETL d√©normalisation (jointures, aggregations, slowly changing dims) |
| **Protocole** | JDBC (AWS DMS Database Migration Service) |
| **Chiffrage** | SSL/TLS over JDBC + AES-256 Redshift cluster encryption |
| **Authentification** | DMS IAM role + Redshift master user |
| **Outil** | AWS DMS (full load + CDC mode) OR AWS Glue (Spark ETL script) |
| **Latency acceptable** | 12-24 hours (nightly OK, not real-time) |
| **RTO/RPO** | RPO = 24h (nightly sync), RTO = rebuild from Flux 4 (S3 backup) + re-transform |
| **Justification non-criticit√©** | Redshift = analytical engine, NOT primary storage. Data can be recalculated from Flux 4 (S3 backup). If Redshift crashes, use S3 + re-run DMS/Glue. |

---

### FLUX 6 : AD ‚Üî Directory Service (üîÑ Temps r√©el - Critique)

| Aspect | Description |
|--------|-------------|
| **Source/Dest** | Active Directory on-premise ‚Üî AWS Managed Microsoft AD |
| **Donn√©es** | M√©tadonn√©es identit√© (usernames, groups, DN entries, SIDs, permissions) |
| **Volume** | ~50 MB (tr√®s petit : metadata only) |
| **Fr√©quence** | Bi-directionnel, temps r√©el (sync change notifications) |
| **Protocol** | LDAP over SSL/TLS (port 636) + AD replication protocol |
| **Chiffrage** | SSL/TLS 1.2+ + Kerberos authentication |
| **Authentication** | Kerberos ticket (AD native auth) |
| **Outil** | AWS Managed AD Connector (lightweight proxy) OR Managed AD (replication) |
| **Latency acceptable** | < 5 seconds (identities must sync quickly) |
| **RTO/RPO** | RPO < 5 sec (real-time SSO), RTO < 1 min (failover to AD Connector) |
| **Justification criticit√©** | SSO = **user authentication on-prem AND cloud must be identical**. If identity sync fails, users cannot authenticate anywhere. CRITICAL. |

---

### FLUX 7 : Services ‚Üí CloudWatch (üëÅÔ∏è Temps r√©el - Non-critique)

| Aspect | Description |
|--------|-------------|
| **Sources** | Redpanda (broker metrics), Redshift (query logs), S3 (request logs), Directory Service (audit) |
| **Destination** | AWS CloudWatch (centralized monitoring) |
| **Donn√©es** | M√©triques de monitoring (CPU %, memory %, disk I/O, network throughput, query count, errors, latency) + CloudTrail audit logs |
| **Volume** | ~5-10 GB/month (aggregated metrics) |
| **Fr√©quence** | Continuous (metrics pushed every 1-5 minutes, logs streamed real-time) |
| **Protocol** | HTTPS CloudWatch API (PutMetricData, PutLogEvents) |
| **Chiffrage** | HTTPS + TLS 1.2+ |
| **Authentication** | IAM roles (service-to-service auth) |
| **Outil** | CloudWatch Agent + CloudTrail (native AWS monitoring) |
| **Latency acceptable** | 1-5 minutes (alerting can tolerate some delay) |
| **RTO/RPO** | RPO = 5 min (metric aggregation window), RTO N/A (monitoring only) |
| **Justification non-criticit√©** | Monitoring = observability only. If CloudWatch fails, infrastructure continues running (no data loss, just no alerts). Important but NOT critical for data integrity. |

---

## R√©sum√© par type de flux

### Flux CRITIQUES (backups + identit√©s)
- **Flux 2** : Redpanda ‚Üí S3 (IoT backup brut)
- **Flux 4** : SQL Server ‚Üí S3 (m√©tier backup brut)
- **Flux 6** : AD ‚Üî Directory Service (identit√©s SSO)

**‚ûú Ces flux DOIVENT r√©ussir, sinon perte donn√©es ou perte acc√®s utilisateurs**

### Flux NON-CRITIQUES (analytique + monitoring)
- **Flux 3** : Redpanda ‚Üí Redshift (aggregations, recalculable)
- **Flux 5** : SQL Server ‚Üí Redshift (analytics, recalculable)
- **Flux 7** : Services ‚Üí CloudWatch (observability, facultatif)

**‚ûú Ces flux peuvent √©chouer court-terme sans perte donn√©es (recalculables depuis Flux 2, 4, 6)**

---

## Matrice : D√©pendances entre flux

| Flux | D√©pend de | Peut √™tre remplac√© par | En cas de failure |
|------|-----------|----------------------|------------------|
| **Flux 2** (Redpanda ‚Üí S3) | Flux 1 (SAN ‚Üí Redpanda) | Aucun | Perte donn√©es IoT temps r√©el |
| **Flux 3** (Redpanda ‚Üí Redshift) | Flux 1 (SAN ‚Üí Redpanda) | Recalcul depuis Flux 2 | Requ√™tes analytics indisponibles |
| **Flux 4** (SQL ‚Üí S3) | SQL Server on-prem | Aucun | Perte donn√©es m√©tier |
| **Flux 5** (SQL ‚Üí Redshift) | Flux 4 (SQL ‚Üí S3) | Recalcul depuis Flux 4 + r√©-ETL | Requ√™tes analytics indisponibles |
| **Flux 6** (AD ‚Üî Dir.Svc) | AD on-prem | AD Connector fallback | Utilisateurs ne peuvent pas se logger cloud |
| **Flux 7** (Services ‚Üí CloudWatch) | Tous les autres | Logs locaux (insuffisant) | Monitoring indisponible (donn√©es safe) |

---

## Recommandations de monitoring & alertes

| Flux | M√©trique √† monitorer | Seuil d'alerte | Action corrective |
|------|-------------------|---------------|----|
| **Flux 1** (SAN ‚Üí Redpanda) | Redpanda lag (partition offset) | > 5 min | V√©rifier r√©seau VPN, Redpanda brokers CPU |
| **Flux 2** (Redpanda ‚Üí S3) | S3 write latency, file count/day | > 10 min, < 100 files | V√©rifier EC2 Spark cluster, S3 rate limits |
| **Flux 3** (Redpanda ‚Üí Redshift) | Redshift ingestion lag, query queue | > 1 hour, > 100 queued | V√©rifier Redshift WLM config, add nodes |
| **Flux 4** (SQL ‚Üí S3) | DataSync task duration, errors | > 2h, any errors | Check SQL Server locks, network bandwidth |
| **Flux 5** (SQL ‚Üí Redshift) | DMS task replication lag, errors | > 30 min, > 0 errors | Check CDC logs, DMS instance CPU/RAM |
| **Flux 6** (AD ‚Üî Dir.Svc) | AD replication lag, sync errors | > 1 min, > 0 errors | Check LDAP connectivity, Directory Service health |
| **Flux 7** (Services ‚Üí CloudWatch) | Log delivery latency, ingestion errors | > 5 min, > 0 errors | Check CloudWatch agent, IAM permissions |

---

**Fin du tableau r√©capitulatif complet.**
